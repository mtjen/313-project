{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c355d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67e1f773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_csv('/Users/mtjen/Desktop/313/project/train.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12123bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 most significant features:\n",
    "# tweet length, TTR, number of punctuation marks, number of periods\n",
    "# avg char per sentence, number adverbs, number of \"to\", number of verbs,\n",
    "# number of entities, tweet entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b868d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b63bf699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non english words from tweets\n",
    "# (trying to get rid of usernames and junk)\n",
    "import nltk\n",
    "# nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "for tweet_index in range(len(train['text'])):\n",
    "    tweet = train.iloc[tweet_index][3]\n",
    "    train.iat[tweet_index, 3] = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) if w.lower() in words or not w.isalpha())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e2d55ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "(7610, 5)\n",
      "       id keyword location                           text  target\n",
      "Index                                                            \n",
      "23     34     NaN      NaN         What a wonderful day !       0\n",
      "24     37     NaN      NaN  No way ... I can ' t eat that       0\n"
     ]
    }
   ],
   "source": [
    "# Remove all data with no content in 'text'\n",
    "empty = np.where(train['text'] == '')\n",
    "print(empty)\n",
    "train.drop(train.index[empty], inplace=True, axis=0)\n",
    "print(train.shape)\n",
    "train = train.assign(Index=range(len(train))).set_index('Index')\n",
    "print(train[23:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5d0241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Features\n",
    "charsToCheck = ['!', '@', '#', '?', '.', ',', 'http']\n",
    "vowels = {'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'}\n",
    "\n",
    "hasLocation = []\n",
    "tweetNumberOfChars = []\n",
    "specialCharacters = []\n",
    "numberOfWords = []\n",
    "avgCharsPerWord = []\n",
    "numNumericTweet = []\n",
    "numLettersTweet = []\n",
    "numUpperTweet = []\n",
    "numVowelsTweet = []\n",
    "numConsonantsTweet = []\n",
    "avgCharsPerSentence = []\n",
    "\n",
    "for index in range(len(train)):\n",
    "    ##### location\n",
    "    location = 0\n",
    "    if pd.isnull(train['location'][index]) == False:\n",
    "            location = 1\n",
    "    \n",
    "    ###### tweet\n",
    "    text = train['text'][index]\n",
    "    # number of characters in twets\n",
    "    numCharsTweet = len(text)\n",
    "    \n",
    "    # number of specific special characters in tweet\n",
    "    specialChars = []\n",
    "    for specialChar in charsToCheck:\n",
    "        numSpecialChar = text.count(specialChar)\n",
    "        specialChars.append(numSpecialChar)\n",
    "    \n",
    "    # average characters per word\n",
    "    words = text.split()\n",
    "    lenWords = []\n",
    "    numNumeric = 0\n",
    "    numLetters = 0\n",
    "    numUpper = 0\n",
    "    numVowels = 0\n",
    "    numConsonants = 0\n",
    "    for word in words:\n",
    "        wordLength = len(word)\n",
    "        lenWords.append(wordLength)\n",
    "        \n",
    "        # number of letters/numbers, uppercase, vowels, consonants\n",
    "        for char in word:\n",
    "            if char.isnumeric():\n",
    "                numNumeric += 1\n",
    "            if char.isalpha():\n",
    "                numLetters += 1\n",
    "                if char.isupper():\n",
    "                    numUpper += 1\n",
    "                if char in vowels:\n",
    "                    numVowels += 1\n",
    "                else:\n",
    "                    numConsonants += 1\n",
    "        \n",
    "    wordLengthAvg = np.mean(lenWords)\n",
    "    \n",
    "    # number of words\n",
    "    numWords = len(words)\n",
    "    \n",
    "    hasLocation.append(location)\n",
    "    tweetNumberOfChars.append(numCharsTweet)\n",
    "    specialCharacters.append(specialChars)\n",
    "    numberOfWords.append(numWords)\n",
    "    avgCharsPerWord.append(wordLengthAvg)\n",
    "    numNumericTweet.append(numNumeric)\n",
    "    numLettersTweet.append(numLetters)\n",
    "    numUpperTweet.append(numUpper)\n",
    "    numVowelsTweet.append(numVowels)\n",
    "    numConsonantsTweet.append(numConsonants)\n",
    "\n",
    "    \n",
    "# specials\n",
    "numEx = []\n",
    "numAt = []\n",
    "numHash = []\n",
    "numQ = []\n",
    "numPeriod = []\n",
    "numComma = []\n",
    "numLinks = []\n",
    "numPunc = []\n",
    "\n",
    "for tweetCharacters in specialCharacters:\n",
    "    totalPunc = 0\n",
    "    for index in range(len(charsToCheck)):\n",
    "        value = tweetCharacters[index]\n",
    "        totalPunc += value\n",
    "        if index == 0:\n",
    "            numEx.append(value)\n",
    "        elif index == 1:\n",
    "            numAt.append(value)\n",
    "        elif index == 2:\n",
    "            numHash.append(value)\n",
    "        elif index == 3:\n",
    "            numQ.append(value)\n",
    "        elif index == 4:\n",
    "            numPeriod.append(value)\n",
    "        elif index == 5:\n",
    "            numComma.append(value)\n",
    "        elif index == 6:\n",
    "            numLinks.append(value)\n",
    "    numPunc.append(totalPunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2c6943ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hasLocation</th>\n",
       "      <th>tweetNumberOfChars</th>\n",
       "      <th>numberOfWords</th>\n",
       "      <th>numEx</th>\n",
       "      <th>numAt</th>\n",
       "      <th>numHash</th>\n",
       "      <th>numQ</th>\n",
       "      <th>numPeriod</th>\n",
       "      <th>numComma</th>\n",
       "      <th>numLinks</th>\n",
       "      <th>numPunc</th>\n",
       "      <th>avgCharsPerWord</th>\n",
       "      <th>numNumericTweet</th>\n",
       "      <th>numLettersTweet</th>\n",
       "      <th>numUpperTweet</th>\n",
       "      <th>numVowelsTweet</th>\n",
       "      <th>numConsonantsTweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.809524</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.388889</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.782609</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.785714</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7610 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hasLocation  tweetNumberOfChars  numberOfWords  numEx  numAt  numHash  \\\n",
       "0               0                  58             12      0      0        1   \n",
       "1               0                  28              6      0      0        0   \n",
       "2               0                  95             20      0      0        0   \n",
       "3               0                  39              8      0      0        1   \n",
       "4               0                  66             15      0      0        2   \n",
       "...           ...                 ...            ...    ...    ...      ...   \n",
       "7605            0                  68             13      0      0        0   \n",
       "7606            0                 100             21      0      2        0   \n",
       "7607            0                  60             18      0      0        0   \n",
       "7608            0                 109             23      0      0        0   \n",
       "7609            0                  66             14      0      0        0   \n",
       "\n",
       "      numQ  numPeriod  numComma  numLinks  numPunc  avgCharsPerWord  \\\n",
       "0        0          0         0         0        1         3.916667   \n",
       "1        0          1         0         0        1         3.833333   \n",
       "2        0          1         0         0        1         3.800000   \n",
       "3        0          0         1         0        2         4.000000   \n",
       "4        0          0         0         0        2         3.466667   \n",
       "...    ...        ...       ...       ...      ...              ...   \n",
       "7605     0          1         0         0        1         4.307692   \n",
       "7606     0          2         0         0        4         3.809524   \n",
       "7607     1          3         0         0        4         2.388889   \n",
       "7608     0          2         0         0        2         3.782609   \n",
       "7609     0          1         0         0        1         3.785714   \n",
       "\n",
       "      numNumericTweet  numLettersTweet  numUpperTweet  numVowelsTweet  \\\n",
       "0                   0               46              4              21   \n",
       "1                   0               22              3              10   \n",
       "2                   0               73              2              32   \n",
       "3                   5               25              0              14   \n",
       "4                   0               50              2              17   \n",
       "...               ...              ...            ...             ...   \n",
       "7605                1               50              7              15   \n",
       "7606                0               75              3              28   \n",
       "7607                9               23              6               6   \n",
       "7608                0               82              3              36   \n",
       "7609                2               44             10              11   \n",
       "\n",
       "      numConsonantsTweet  target  \n",
       "0                     25       1  \n",
       "1                     12       1  \n",
       "2                     41       1  \n",
       "3                     11       1  \n",
       "4                     33       1  \n",
       "...                  ...     ...  \n",
       "7605                  35       1  \n",
       "7606                  47       1  \n",
       "7607                  17       1  \n",
       "7608                  46       1  \n",
       "7609                  33       1  \n",
       "\n",
       "[7610 rows x 18 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new dataframe to hold features\n",
    "colNames = ['hasLocation', 'tweetNumberOfChars', 'numberOfWords', 'numEx', 'numAt', \n",
    "            'numHash', 'numQ', 'numPeriod', 'numComma', 'numLinks', 'numPunc',\n",
    "            'avgCharsPerWord', 'numNumericTweet', 'numLettersTweet', 'numUpperTweet',\n",
    "            'numVowelsTweet', 'numConsonantsTweet']\n",
    "colValues = [hasLocation, tweetNumberOfChars, numberOfWords, numEx, numAt, numHash,\n",
    "             numQ, numPeriod, numComma, numLinks, numPunc, avgCharsPerWord, numNumericTweet, \n",
    "             numLettersTweet, numUpperTweet, numVowelsTweet, numConsonantsTweet]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for index in range(len(colNames)):\n",
    "    colName = colNames[index]\n",
    "    colVals = colValues[index]\n",
    "    data[colName] = colVals\n",
    "\n",
    "    \n",
    "data['target'] = train['target']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3f164d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "\n",
      "      00  000  0000  007npen6lg  00cy9vxeff  00end  00pm  01  02  0215  ...  \\\n",
      "0      0    0     0           0           0      0     0   0   0     0  ...   \n",
      "1      0    0     0           0           0      0     0   0   0     0  ...   \n",
      "2      0    0     0           0           0      0     0   0   0     0  ...   \n",
      "3      0    1     0           0           0      0     0   0   0     0  ...   \n",
      "4      0    0     0           0           0      0     0   0   0     0  ...   \n",
      "...   ..  ...   ...         ...         ...    ...   ...  ..  ..   ...  ...   \n",
      "7605   0    0     0           0           0      0     0   0   0     0  ...   \n",
      "7606   0    0     0           0           0      0     0   0   0     0  ...   \n",
      "7607   0    0     0           0           0      0     0   1   0     0  ...   \n",
      "7608   0    0     0           0           0      0     0   0   0     0  ...   \n",
      "7609   0    0     0           0           0      0     0   0   0     0  ...   \n",
      "\n",
      "      û_1  û_ahhh  û_https  û_one  û_t  û_that  û_turns  û_you  ûª93  ûò800000  \n",
      "0       0       0        0      0    0       0        0      0     0         0  \n",
      "1       0       0        0      0    0       0        0      0     0         0  \n",
      "2       0       0        0      0    0       0        0      0     0         0  \n",
      "3       0       0        0      0    0       0        0      0     0         0  \n",
      "4       0       0        0      0    0       0        0      0     0         0  \n",
      "...   ...     ...      ...    ...  ...     ...      ...    ...   ...       ...  \n",
      "7605    0       0        0      0    0       0        0      0     0         0  \n",
      "7606    0       0        0      0    0       0        0      0     0         0  \n",
      "7607    0       0        0      0    0       0        0      0     0         0  \n",
      "7608    0       0        0      0    0       0        0      0     0         0  \n",
      "7609    0       0        0      0    0       0        0      0     0         0  \n",
      "\n",
      "[7610 rows x 11636 columns]\n",
      "\n",
      "TD-IDF Vectorizer\n",
      "\n",
      "       00      000  0000  007npen6lg  00cy9vxeff  00end  00pm        01   02  \\\n",
      "0     0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "1     0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "2     0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "3     0.0  0.52357   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "4     0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "...   ...      ...   ...         ...         ...    ...   ...       ...  ...   \n",
      "7605  0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "7606  0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "7607  0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.348452  0.0   \n",
      "7608  0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "7609  0.0  0.00000   0.0         0.0         0.0    0.0   0.0  0.000000  0.0   \n",
      "\n",
      "      0215  ...  û_1  û_ahhh  û_https  û_one  û_t  û_that  û_turns  û_you  \\\n",
      "0      0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "1      0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "2      0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "3      0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "4      0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "...    ...  ...  ...     ...      ...    ...  ...     ...      ...    ...   \n",
      "7605   0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "7606   0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "7607   0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "7608   0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "7609   0.0  ...  0.0     0.0      0.0    0.0  0.0     0.0      0.0    0.0   \n",
      "\n",
      "      ûª93  ûò800000  \n",
      "0      0.0       0.0  \n",
      "1      0.0       0.0  \n",
      "2      0.0       0.0  \n",
      "3      0.0       0.0  \n",
      "4      0.0       0.0  \n",
      "...    ...       ...  \n",
      "7605   0.0       0.0  \n",
      "7606   0.0       0.0  \n",
      "7607   0.0       0.0  \n",
      "7608   0.0       0.0  \n",
      "7609   0.0       0.0  \n",
      "\n",
      "[7610 rows x 11636 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize contents of all tweets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "countvectorizer = CountVectorizer(analyzer ='word', stop_words='english')\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "\n",
    "count_wm = countvectorizer.fit_transform(train['text'])\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(train['text'])\n",
    "\n",
    "#retrieve the terms found in the corpora\n",
    "count_tokens = countvectorizer.get_feature_names()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "\n",
    "df_countvect = pd.DataFrame(data = count_wm.toarray(),index = list(train.index.values) ,columns = count_tokens)\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = list(train.index.values) ,columns = tfidf_tokens)\n",
    "\n",
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_countvect)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0bd7e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7610, 11610)\n",
      "(7610, 11610)\n"
     ]
    }
   ],
   "source": [
    "# Remove entries with junk characters\n",
    "# dont really know why they show up\n",
    "def isascii(s):\n",
    "    \"\"\"Check if the characters in string s are in ASCII, U+0-U+7F.\"\"\"\n",
    "    return len(s) == len(s.encode())\n",
    "\n",
    "for col in list(df_countvect.columns):\n",
    "    if isascii(col) == False:\n",
    "        df_countvect = df_countvect.drop([col], axis=1)\n",
    "        df_tfidfvect = df_tfidfvect.drop([col], axis=1)\n",
    "\n",
    "print(df_countvect.shape)\n",
    "print(df_tfidfvect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff7265b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned vectorized tweets to csv\n",
    "# print(df_tfidfvect.iloc[0])\n",
    "# df_tfidfvect.to_csv('vectorized_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed7ffca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TTR (text to token ratio)\n",
    "# occurances = list()\n",
    "# for tweet in range(len(df_tfidfvect)):\n",
    "#     count = 0\n",
    "#     for word in df_tfidfvect.iloc[tweet]:\n",
    "#         if word != 0:\n",
    "#             count+=df_tfidfvect.iloc[tweet]\n",
    "#     occurances.append(count)\n",
    "\n",
    "# count = np.sum(df_tfidfvect, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bb2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7227f423",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17352/3789386526.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mttr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moccurances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0moccurances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mttr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1537\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1538\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# ttr = []\n",
    "# for i in range(len(occurances)):\n",
    "#     if occurances[i] ==0:\n",
    "#         ttr.append(0)\n",
    "#         continue\n",
    "#     ttr.append(occurances[i]/count[i])\n",
    "# print(ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903da3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make heatmap to visualize\n",
    "import seaborn as sns\n",
    "sns.heatmap(features.corr(), cmap='BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e8196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train and test data\n",
    "trainData = data.sample(frac = 0.8, random_state = 25)\n",
    "testData = data.drop(trainData.index)\n",
    "\n",
    "stopIndex = data.shape[1] - 1\n",
    "\n",
    "trainArray = trainData.values\n",
    "trainX = trainArray[:,0:stopIndex]\n",
    "trainY = trainArray[:,stopIndex]\n",
    "\n",
    "testArray = testData.values\n",
    "testX = testArray[:,0:stopIndex]\n",
    "testY = testArray[:,stopIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3d065e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6999343401181878"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run random trees\n",
    "model = RandomForestClassifier()\n",
    "model.fit(trainX, trainY)\n",
    "model.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed7ffca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7227f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 21362)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab165a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
